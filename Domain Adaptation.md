---
Reference:
  - https://areganti.notion.site/Week-1-Part-2-Domain-and-Task-Adaptation-Methods-6ad3284a96a241f3bd2318f4f502a1da
---
## Pre-Training

Takes days to weeks to months. Uses large set of data (for domain training) - can customize the model's architecture, size, tokenizer, etc,.

Pre-trained on extensive datasets.

Examples of domain-specific pre-training include models like ESMFold, ProGen2 for protein sequences, Galactica for science, BloombergGPT for finance, and StarCoder for code

---
## Fine-Tuning
Takes minutes to hours.
